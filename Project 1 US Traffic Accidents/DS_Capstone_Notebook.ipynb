{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MO1GJ_-kbqCB"
   },
   "source": [
    "![Add a relevant banner image here](path_to_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efdDK3eIbqCC"
   },
   "source": [
    "# **Flatiron Data Science Capstone Project 1: US Traffic Accidents**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6ge5WeibqCC"
   },
   "source": [
    "## Overview\n",
    "\n",
    "text here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyLQUFdUbqCC"
   },
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5b80PSYbqCC"
   },
   "source": [
    "The US Department of Transportation (DOT) is concerned about the number of traffic accidents across the US and wants to develop strategies to reduce accidents and improve road safety.\n",
    "\n",
    "For the Department of Transportation (DOT), understanding and reducing traffic accidents is a critical mission that directly impacts public safety, economic costs, and quality of life across the United States.\n",
    "\n",
    "This analysis matters from a stakeholder and business perspective:\n",
    "\n",
    "-Economic Impact: Traffic accidents cost billions annually in medical expenses, property damage, and lost productivity, making even small reductions highly valuable.\n",
    "\n",
    "-Public Safety: As a leading cause of injury and death, reducing traffic accidents directly fulfills DOT's core mandate to protect citizens.\n",
    "\n",
    "-Infrastructure Prioritization: Data analysis enables strategic allocation of limited infrastructure improvement budgets to highest-risk areas.\n",
    "\n",
    "-Policy Development: Accident data informs new safety regulations and provides metrics to evaluate existing programs' effectiveness.\n",
    "\n",
    "-Stakeholder Accountability: Comprehensive analysis demonstrates evidence-based decision-making to Congress, local governments, and the public.\n",
    "\n",
    "-Cross-Agency Collaboration: Shared data insights can align accident reduction efforts across DOT, law enforcement, and emergency services.\n",
    "\n",
    "-Technology Integration: Understanding accident patterns guides how emerging vehicle technologies should be regulated to maximize safety benefits.\n",
    "\n",
    "This project supports DOT's mission by translating complex accident data into actionable insights. By identifying key patterns and risk factors, it empowers smarter infractstructure investment, better regulation and ultimately safer roads for all.\n",
    "\n",
    "Project Objectives:\n",
    "1. Identify accident hotspots: This analysis will determine when and where accidents most frequently occur.  Patterns such as time of day, day of week, season, and geographic location will be examined to determine of there are critical hotspots and time periods that may warrant intervention.\n",
    "2. Analyze environmental risk factors: This analysis will determine how weather conditions correlate with accident rates. Factors such as visibility, precipitation, temperature and other environmental variables will be examined to assess their impact on driver behavior and road conditions. The goal is to determine if certain weather conditions should trigger early warning notifications to drivers.\n",
    "3. Identify infrastructure considerations: This analysis will determine how specific road features are associated with accident severity. This will include road design, signage, lighting, and other infrastructural elements that could contribute to or mitigate accident risk\n",
    "\n",
    "By successfully identifying accident hotspots, environmental risk factors, and infracture considerations, DOT can execute initiatives to address these issues and fulfill its mission to the public."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clc1XoCJbqCC"
   },
   "source": [
    "# Data Understanding\n",
    "\n",
    "Text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding - Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14751,
     "status": "ok",
     "timestamp": 1751868392582,
     "user": {
      "displayName": "Joseph Lee",
      "userId": "03614348732629050745"
     },
     "user_tz": 420
    },
    "id": "yDRt10wpcpz7",
    "outputId": "fbc84979-db04-4e82-9a4d-df6405dda52c"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 1456,
     "status": "ok",
     "timestamp": 1751868402022,
     "user": {
      "displayName": "Joseph Lee",
      "userId": "03614348732629050745"
     },
     "user_tz": 420
    },
    "id": "f2YRw_gAbqCC"
   },
   "outputs": [],
   "source": [
    "# Load relevant imports here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels as stats\n",
    "import scipy\n",
    "import holidays\n",
    "\n",
    "us_holidays = holidays.UnitedStates()  # Create a US holidays object to check for holidays\n",
    "\n",
    "# column definitions in Onedrive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O7y2MgDLbqCC"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "# df = pd.read_csv('/content/drive/MyDrive/US_Accidents_March23.csv')\n",
    "\n",
    "# PC path to the CSV file\n",
    "# df = pd.read_csv(r\"C:\\Users\\jtlee\\OneDrive\\Documents\\Flatiron Schoolwork\\DS_11 Capstone\\Project 1 US Traffic Accidents\\US_Accidents_March23.csv\")\n",
    "\n",
    "# add laptop path to CSV file\n",
    "df = pd.read_csv(r'C:\\Users\\eeleoj62\\OneDrive\\Documents\\Flatiron Schoolwork\\DS_11 Capstone\\Project 1 US Traffic Accidents\\US_Accidents_March23.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nShape of US accidents dataset: {df.shape}\\n\")     # (rows, columns)\n",
    "\n",
    "df.info()  # DataFrame info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Print preview of the dataframe:\\n\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert severity to categories (for EDA and readibility)\n",
    "severity_map = {\n",
    "    1: 'Low',\n",
    "    2: 'Moderate', \n",
    "    3: 'High',\n",
    "    4: 'Severe'\n",
    "}\n",
    "\n",
    "df['Severity_Level'] = df['Severity'].map(severity_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding - Objective 1 Accident hotspot analysis\n",
    "\n",
    "Will look at patterns such as time of day, day of week, season, and geogrpahic location\n",
    "\n",
    "Temporal features:\n",
    "Start time can be used to determine time of day categorized into periods such as morning, afternoon and evening to assess whether accidents occur during certain windows. Day of week - accidents may increase on certain weekdays such as Fridays due to end of week fatigure or increased traffic. Season - month of the year will be mapped to seasons to explore weather or seasonal patterns. using US federal holiday calendars we will tag whether accidents occurred on or during holidays when traffic and risk behaviors may increase \n",
    "\n",
    "Geographic features:\n",
    "Geographic location will be determined using state, county and zip code. This will help identify regions with higher accident concentrations, supporting hotspot detection at multiple geographic levels.\n",
    "\n",
    "Together, these temporal and spatial insights will help the DOT pinpoint high-risk times and locations, enabling targeted interventions such as increased patrols, public service campaigns, or infrastructure investment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe for objective 1 accident hotspot analysis\n",
    "\n",
    "df_hotspots = df[['ID', 'Severity_Level', 'Start_Time', 'City', 'County', 'State', 'Zipcode', ]].copy()\n",
    "\n",
    "print(f\"Shape of hotspots df: {df_hotspots.shape}\")\n",
    "print(df_hotspots.info())\n",
    "print(df_hotspots.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "253 missing City and 1,915 zipcode values out of 7.7 million data points is not significant. They will be dropped, as they shouldn't hurt the analysis at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for unique states and their counts\n",
    "print(f\"\\nUnique states: {sorted(df['State'].unique())}\")\n",
    "print(f\"\\nNumber of states included in the dataset: {df['State'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that there are only 49 states included in this US accidents dataset. Upon further examination, DC has been included, and Alaska and Hawaii are not present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding - Objective 2 Environmental Risk Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will look at environmental factors such as temperature, wind chill, humidity, pressure, visibility, wind speed, precipitation, and weather condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe for objective 1 accident hotspot analysis\n",
    "\n",
    "df_weather = df[['ID', 'Severity_Level', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)', 'Weather_Condition' ]].copy()\n",
    "\n",
    "print(f\"Shape of weather df: {df_weather.shape}\")\n",
    "print(df_weather.info())\n",
    "print(df_weather.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing values, most notably Wind_Chill and Precipitation, and to an extent wind speed\n",
    "\n",
    "Temperature -> impute with median temp\n",
    "Wind_chill -> impute with Temperature\n",
    "Humidity -> impute with median humidity\n",
    "Pressure -> impute with median pressure\n",
    "visibility -> impute with median visibility\n",
    "wind speed -> conditional median imputation \n",
    "df['Wind_Speed(mph)'] = df.groupby('Weather_Condition')['Wind_Speed(mph)']\\\n",
    "                          .transform(lambda x: x.fillna(x.median()))\n",
    "precipitation -> impute 0 (no rain)\n",
    "weather condition -> impute 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize weather distributions\n",
    "\n",
    "weather_cols = ['Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)', 'Precipitation(in)']\n",
    "\n",
    "# Set up subplots: 4 rows, 2 columns (adjust layout as needed)\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(15, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each weather feature in a subplot\n",
    "for i, col in enumerate(weather_cols):\n",
    "    sns.histplot(df_weather[col].dropna(), bins=20, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {col}')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "# Remove empty subplot if cols < subplots\n",
    "for j in range(len(weather_cols), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for outliers in temperature, pressure, visiblity, wind speed, and precipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize weather conditions\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "top_weather = df['Weather_Condition'].value_counts().head(20)\n",
    "\n",
    "sns.barplot(x=top_weather.index, y=top_weather.values)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Top 20 Weather Conditions')\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Weather Condition')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding - Objective 3 Infrastructure Considerations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine infrastructure data for the bumps, crossings, yield signs, junctions, dead-ends/no-exit roads, railway crossings, roundabouts, stations (train, gas, etc.), stop signs, traffic calming features, traffic lights, and turning loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataframe for objective 3 infrastructure analysis\n",
    "\n",
    "df_infra = df[['ID', 'Severity_Level', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop']].copy()\n",
    "\n",
    "print(f\"Shape of infrastructure df: {df_infra.shape}\")\n",
    "print(df_infra.info())\n",
    "print(df_infra.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize presence of infrastructure\n",
    "\n",
    "infra_cols = ['Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop']\n",
    "\n",
    "infra_counts = df_infra[infra_cols].sum().sort_values()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "infra_counts.plot(kind='barh')\n",
    "plt.title('Number of Accidents with Infrastructure Feature Present')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Infrastructure Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKrFXnZzbqCC"
   },
   "source": [
    "# Data Preparation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep - Objective 1 Accident hotspot analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with missing City and Zipcode values\n",
    "df_hotspots.dropna(subset=['City', 'Zipcode'], inplace=True)\n",
    "df_hotspots.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take Start Time column and convert to datetime format\n",
    "df_hotspots['Start_Time'] = pd.to_datetime(df_hotspots['Start_Time'], format='mixed', errors='coerce')\n",
    "\n",
    "# create new column for time of day (hour of the day)\n",
    "df_hotspots['Time_of_day'] = df_hotspots['Start_Time'].dt.hour\n",
    "\n",
    "# create new column for day of week\n",
    "# df_hotspots['day_of_week'] = df_hotspots['Start_Time'].dt.dayofweek\n",
    "\n",
    "# convert day of week to actual day name\n",
    "df_hotspots['Day_of_week'] = df_hotspots['Start_Time'].dt.day_name()\n",
    "\n",
    "# Create a new column for whether the accident occurred on a holiday\n",
    "df_hotspots['Is_Holiday'] = df_hotspots['Start_Time'].apply(\n",
    "    lambda x: x.date() in us_holidays if pd.notnull(x) else False\n",
    ")\n",
    "\n",
    "print(df_hotspots.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to get the season based on the month\n",
    "def get_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Fall'\n",
    "    \n",
    "df_hotspots['Season'] = df_hotspots['Start_Time'].dt.month.apply(get_season)\n",
    "\n",
    "print(df_hotspots.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check of consistency of spelling in top 50 city and county columns\n",
    "print(df_hotspots['City'].value_counts().head(50))\n",
    "print(df_hotspots['County'].value_counts().head(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract just the first 5 digits of the Zipcode column\n",
    "# This is to ensure that the Zipcode column is in the correct format\n",
    "df_hotspots['Zipcode'] = df_hotspots['Zipcode'].astype(str).str.extract(r'^(\\d{5})')\n",
    "print(df_hotspots['Zipcode'].value_counts().head(50))\n",
    "print(df_hotspots['Zipcode'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_hotspots.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group accidents by time of day, group further into periods of the day\n",
    "def convert_hour_to_period(hour):\n",
    "    if 0 <= hour < 6:\n",
    "        return 'Late Night'\n",
    "    elif 6 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "df_hotspots['Time_period'] = df_hotspots['Time_of_day'].apply(convert_hour_to_period)\n",
    "print(df_hotspots.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examining value counts for the time periods\n",
    "df_hotspots['Time_period'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examining value counts for days of the week\n",
    "df_hotspots['Day_of_week'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examining value counts for holidays\n",
    "df_hotspots['Is_Holiday'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_hotspots.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep - Objective 2 Environmental risk \n",
    "\n",
    "Temperature -> impute with median temp\n",
    "Wind_chill -> impute with Temperature\n",
    "Humidity -> impute with median humidity\n",
    "Pressure -> impute with median pressure\n",
    "visibility -> impute with median visibility\n",
    "wind speed -> conditional median imputation \n",
    "df['Wind_Speed(mph)'] = df.groupby('Weather_Condition')['Wind_Speed(mph)']\\\n",
    "                          .transform(lambda x: x.fillna(x.median()))\n",
    "precipitation -> impute 0 (no rain)\n",
    "weather condition -> impute 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine outliers in temperature\n",
    "print(\"\\nTemperature statistics:\")\n",
    "\n",
    "print(df_weather['Temperature(F)'].describe())\n",
    "\n",
    "Q1 = df_weather['Temperature(F)'].quantile(0.25)\n",
    "Q3 = df_weather['Temperature(F)'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# filter out outliers\n",
    "df_weather = df_weather[df_weather['Temperature(F)'] <= 130]  # Ensure no extreme outliers above 130, anything over 130 is highly unlikely in the US\n",
    "\n",
    "df_weather = df_weather[df_weather['Temperature(F)'] >= -60]  # Ensure no extreme outliers below -60, anything under is highly unlikely in the continental US\n",
    "\n",
    "print(df_weather['Temperature(F)'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine outliers in pressure\n",
    "print(\"\\nPressure statistics:\")\n",
    "print(df_weather['Pressure(in)'].describe())\n",
    "Q1 = df_weather['Pressure(in)'].quantile(0.25)\n",
    "Q3 = df_weather['Pressure(in)'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "# filter out outliers\n",
    "df_weather = df_weather[df_weather['Pressure(in)'] <= upper_bound]\n",
    "print(df_weather['Pressure(in)'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine outliers in visibility\n",
    "print(\"\\nVisibility(mi) statistics:\")\n",
    "\n",
    "print(df_weather['Visibility(mi)'].describe())\n",
    "\n",
    "Q1 = df_weather['Visibility(mi)'].quantile(0.25)\n",
    "Q3 = df_weather['Visibility(mi)'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out outliers\n",
    "df_weather = df_weather[df_weather['Visibility(mi)'] <= upper_bound]\n",
    "\n",
    "print(df_weather['Visibility(mi)'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine outliers in wind speed\n",
    "print(\"\\nWind_Speed(mph) statistics:\")\n",
    "\n",
    "print(df_weather['Wind_Speed(mph)'].describe())\n",
    "\n",
    "Q1 = df_weather['Wind_Speed(mph)'].quantile(0.25)\n",
    "Q3 = df_weather['Wind_Speed(mph)'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "df_weather = df_weather[df_weather['Wind_Speed(mph)'] <= 100] # Ensure no extreme outliers above 100 mph, anything over is highly unlikely in the US\n",
    "\n",
    "print(df_weather['Wind_Speed(mph)'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize weather again to ensure outliers have been removed\n",
    "# Set up subplots: 4 rows, 2 columns (adjust layout as needed)\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(15, 20))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot each weather feature in a subplot\n",
    "for i, col in enumerate(weather_cols):\n",
    "    sns.histplot(df_weather[col].dropna(), bins=20, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {col}')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "# Remove empty subplot if cols < subplots\n",
    "for j in range(len(weather_cols), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature -> impute with median temp\n",
    "Wind_chill -> impute with Temperature\n",
    "Humidity -> impute with median humidity\n",
    "Pressure -> impute with median pressure\n",
    "visibility -> impute with median visibility\n",
    "wind speed -> conditional median imputation \n",
    "df['Wind_Speed(mph)'] = df.groupby('Weather_Condition')['Wind_Speed(mph)']\\\n",
    "                          .transform(lambda x: x.fillna(x.median()))\n",
    "precipitation -> impute 0 (no rain)\n",
    "weather condition -> impute 'Unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values in temperature column with median temperature\n",
    "df_weather['Temperature(F)'].fillna(df_weather['Temperature(F)'].median(), inplace=True)\n",
    "\n",
    "# impute missing values in wind chill column with median wind chill\n",
    "df_weather['Wind_Chill(F)'].fillna(df_weather['Wind_Chill(F)'].median(), inplace=True)\n",
    "\n",
    "# impute missing values in humidity column with median humidity\n",
    "df_weather['Humidity(%)'].fillna(df_weather['Humidity(%)'].median(), inplace=True)\n",
    "\n",
    "# impute missing values in pressure with median pressure\n",
    "df_weather['Pressure(in)'].fillna(df_weather['Pressure(in)'].median(), inplace=True)\n",
    "\n",
    "# impute missing values in visibility with median visibility\n",
    "df_weather['Visibility(mi)'].fillna(df_weather['Visibility(mi)'].median(), inplace=True)\n",
    "\n",
    "# impute missing values in wind speed with median wind speed\n",
    "df_weather['Wind_Speed(mph)'] = df_weather.groupby('Weather_Condition')['Wind_Speed(mph)']\\\n",
    "                          .transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "# impute missing values in precipitation column with 0 (no rain)\n",
    "df_weather['Precipitation(in)'].fillna(0, inplace=True)\n",
    "\n",
    "# impute missing values in weather condition column with 'Clear'\n",
    "df_weather['Weather_Condition'].fillna('Clear', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_weather.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep - Objective 3 Infrastructure considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every infrastructure column (bool type) is completed. No imputation or rows drops are required.There are no outliers because bool is just True/False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert booleans to integers\n",
    "df_infra[infra_cols] = df_infra[infra_cols].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_infra.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQBIXW0UbqCC"
   },
   "source": [
    "# Analysis\n",
    "\n",
    "Text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bz5RLgt2bqCC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZVNWNWMbqCD"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "## Business Insight/Recommendation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5tbWAotbqCD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuWM8ooibqCD"
   },
   "source": [
    "## Business Insight/Recommendation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8o9pEkCJbqCD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwNpKS-GbqCD"
   },
   "source": [
    "## Business Insight/Recommendation 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPcozzicbqCD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8uNU1wvbqCD"
   },
   "source": [
    "### Tableau Dashboard link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6iIwf-WbqCD"
   },
   "source": [
    "## Conclusion and Next Steps\n",
    "Text here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
