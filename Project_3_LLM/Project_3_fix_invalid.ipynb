{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/eeleoj62/DS_capstone/blob/main/Project_3_LLM/Project_3_fix_invalid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULwlp0t2cOwR"
   },
   "source": [
    "**next steps:**\n",
    "* try pegasus\n",
    "* remove overly short dialogues or summaries (<10 words) [COMPLETE]\n",
    "* try learning rates like 3e-5, 1e-4 [SKIP]\n",
    "* use cosine learning rate scheduler [COMPLETE]\n",
    "* increase num_beams (4-6) for faster training. 10 takes up too much GPU memory. keep at 4 [COMPLETE]\n",
    "* early stopping trainer = trainer (...,callbacks=[EarlyStoppingCallback(early_stopping_patience=2)] [SKIP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xhn1JOvkRHvF"
   },
   "source": [
    "# Project 3 - Supervised Learning for Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8aSuR7fhRQIJ"
   },
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSlpXrPRRUEa"
   },
   "source": [
    "### Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRgx41lQRV9S"
   },
   "source": [
    "### Dataset Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NnNfsfZ7RZbG"
   },
   "source": [
    "### Project Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KP-kvP28RcZH"
   },
   "source": [
    "### Project Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocUv3H-WRfHh"
   },
   "source": [
    "## Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZkQD5lEqsXF3",
    "outputId": "c0cadc88-bc5c-42e6-bc4a-a98214166396"
   },
   "outputs": [],
   "source": [
    "# install mlflow, evaluate, rouge_score and datasets\n",
    "!pip install -q mlflow\n",
    "!pip install -q evaluate\n",
    "!pip install -q rouge_score\n",
    "\n",
    "# install hugging face\n",
    "!pip install -q datasets\n",
    "\n",
    "!pip install -q ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dmfe1VrLjAe1",
    "outputId": "c251f5d7-6f00-467b-8529-76d1d33472dc"
   },
   "outputs": [],
   "source": [
    "# mount Google Drive to save models and datasets\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDyPy2jEFkGF"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, \\\n",
    "  accuracy_score, ConfusionMatrixDisplay\n",
    "from transformers import BertTokenizer, EncoderDecoderModel, TrainingArguments,\\\n",
    "  Trainer, DataCollatorForSeq2Seq\n",
    "import transformers\n",
    "from transformers import EarlyStoppingCallback\n",
    "import mlflow\n",
    "import mlflow.transformers\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "# set random seed for reproducibility\n",
    "def set_seed(seed_value=42):\n",
    "  random.seed(seed_value)\n",
    "  np.random.seed(seed_value)\n",
    "  os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383,
     "referenced_widgets": [
      "25a6f7179470495a9056c298fa1d7c22",
      "1374390b206544a2abec47c41679fd7d",
      "0eb15f42588f4c1e8f5caebeafab3dc7",
      "454e72f4b22d4df1a9b6a2f3b6aadacd",
      "b0b29603af6a4b1eb371953dedcee257",
      "cfc9809466e04c3cacd6cf9da3927701",
      "abecf233ef214fafb73bbe811f625e67",
      "b7b888a089934d259a1f8779523beb66",
      "af4636af1c7d4cbda16eeeb14dd6ab3b",
      "052af3a4b85d46e28d2f5a7681d24428",
      "e656b5f95f74439caaf14088cd42e079",
      "f3ea1eba35f14bbfb545e7122ca971c8",
      "cd2cf262881c409c8aebc1f9fe2ee2bf",
      "b94f57ed81c5461ea7a60db0850e8393",
      "81968b91e03d4f8fb727f7756baa0847",
      "ed2631f62ccd436bb52f0aaafbf59747",
      "5d5d07b2f91543728dd36ffcf3d80001",
      "43a16093b6004f1286c885a470ece6d2",
      "9c8ca6f2699942e1ad2ec818cd9e7f28",
      "d2487e59b9e04807b8d0553755cf7cd5",
      "53a8bb34ac434f298135e0369d50a52e",
      "38684a8398c54c87bc4cb9109ca3f1d0",
      "965e40ce5434459f96c02ce2ea2ef999",
      "344ad18cebe44ccb8c0c398e65c014a2",
      "60a1c2602f1745ddba6e41b064f5caa5",
      "e9ee4749a3d241d389fbb6500cde7a9f",
      "aade28e17cea421aa35697cdbfb904ad",
      "bce44c8ac19142b0b5711fa5ae1d99b0",
      "5479ef5148844f0ba0a36668adec9bab",
      "526b773cab264e3ab1ebb95225d316c5",
      "01698c9cc740441da1fd8fca3ab5f01e",
      "5743dc3fbf0f4acc91f337ef764167e1",
      "17fc68f2c18443feb79fcea38e04b2e1",
      "027e0b9cbd5c4cb9b9a74f99320c98c8",
      "7fea42c3d6274ac5a0f0437ef423d09f",
      "f8d082f11e92444ebe05d898e9a49c28",
      "3a7610d63aaa482e94507647f0aff113",
      "881abb495f284a83b6f102d99aeae996",
      "92aa654210ff45f7a0c3306b62023b41",
      "1da36e059dfc4aafa5496cf15b3b9169",
      "8e245b47357647f6a37c40f50b7be9d8",
      "35201bca37ce49c98ab9229a24109e20",
      "b675eaa6767049e49d0cfecaf961c702",
      "64cf9efca1db4a308292d0598294cd19",
      "bc411462bf7c4967a26c4e0e33670f81",
      "a006966beeba461c890d9aff0c6dc625",
      "1888ec9092524071a6438b8ccef2a8f6",
      "73c18bb4fc5e4b908d7f26356eb9ba15",
      "14e1980f7f584b28a1b824f6be610a64",
      "1f822bc46a4045a1b5504bba2c995495",
      "c1c6981922074adead43663d5bf39e6f",
      "a397670065f9480e89a49155e8cacf65",
      "5362a37dc230410cad9aceeb991dd1e6",
      "34551fa5276e43fca076c384033519b4",
      "24ab5f3d39f24daea3e4fc837b2a8676",
      "eec354edd270452990b545be3517ef0a",
      "184c57e0161e48adbe2fbf36240e9bf4",
      "123d945600064965b55947a55c809ff4",
      "d78e7c686ce24078ae6a7c609173b370",
      "c9e08265af3b4482b9660c1d05eeec90",
      "8ebf8406b47944bba6bb9351fa55e0b7",
      "4daf186a86514182bb634d6cdfeddb8a",
      "0035565d7aba4411a2648a2a05c7cbee",
      "c63ad037e280491f9427822964023fad",
      "276fcced9b3b457db6ef59a8ccc3456a",
      "2a72941b96484786911396d8791ae5d7",
      "f286c0b840204fdfbb36ac81194d4aec",
      "37e92d5adf854f1dacdab3fba3bbd175",
      "11798bc9638248c1ae676e0b9ee990fb",
      "6c8ab30caad14b01b2e118eab1614572",
      "ab134059e40b41cc8b5863972e98a539",
      "333ec3fa25374af3829cca74e830e28a",
      "57b92c7f18e1454fb58406902920e9d7",
      "3f86aab3cd524970bd138d12ef21ffaf",
      "a3637773cd004b17be42afb9168692f1",
      "0c4e6137db98414da73d61c38a040da8",
      "9627686f68114979a7f15dc78ec1acbf"
     ]
    },
    "id": "Saw8v1huEsgn",
    "outputId": "7041cb02-c755-4c9f-89f8-572d2ddcff04"
   },
   "outputs": [],
   "source": [
    "# load samsum dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# time the dataset loading\n",
    "start_time = time.time()\n",
    "\n",
    "# load samsum dataset\n",
    "dataset = load_dataset('knkarthick/samsum')\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Dataset loaded in {(end_time - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wSGnIl87E04x",
    "outputId": "1c379cc9-6840-4beb-9265-e8e8d8fae6a3"
   },
   "outputs": [],
   "source": [
    "# explore the samsum dataset structure\n",
    "print(f\"Details of samsum dataset:\\n\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9T1qUJgoFAfu",
    "outputId": "d8d59078-69d4-4f16-a08b-6b152dc2821b"
   },
   "outputs": [],
   "source": [
    "# print first line from the train split of the samsum dataset\n",
    "\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7QGjNoW7FEyq",
    "outputId": "e3ea519c-1a80-4aac-cd46-37271788160b"
   },
   "outputs": [],
   "source": [
    "# convert train, validation and test splits to dataframes for EDA\n",
    "# pandas dfs are better for quick exploration than Hugging Face datasets\n",
    "# hugging face dataset objects are good for modeling, but less flexible for:\n",
    "# describe(), filtering, plotting, sample inspection\n",
    "\n",
    "df_train = pd.DataFrame(dataset['train'])\n",
    "print(f\"First 5 lines of the train split of SAMsum dataset:\\n\", df_train.head())\n",
    "\n",
    "df_val = pd.DataFrame(dataset['validation'])\n",
    "print(f\"\\nFirst 5 lines of the validation split of SAMsum dataset:\\n\", df_val.head())\n",
    "\n",
    "df_test = pd.DataFrame(dataset['test'])\n",
    "print(f\"\\nFirst 5 lines of the test split of SAMsum dataset:\\n\", df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGSTKOAQTOOP"
   },
   "source": [
    "### Text Exploration\n",
    "Analyze the characteristics of the dialogues and summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AuJvmskFUkRr",
    "outputId": "11db84c8-facf-4a98-d8b9-1c4960c383b2"
   },
   "outputs": [],
   "source": [
    "# train df basic info and describe\n",
    "print(df_train.info())\n",
    "print(df_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5xScGQdqU1_l",
    "outputId": "a11562f5-3dcf-44e5-d8b2-72dd9e9fa0e9"
   },
   "outputs": [],
   "source": [
    "# length of train df\n",
    "print(f\"Length of train df: {len(df_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_T1kwXUGQO0",
    "outputId": "25bc023e-1ac0-4950-a135-3df371686da9"
   },
   "outputs": [],
   "source": [
    "# text length analysis\n",
    "# analysis will include speaker names\n",
    "\n",
    "# create a new column 'dialogue_length' in df_train by taking 'dialogue' column and splitting the string and finding its length\n",
    "df_train['dialogue_length'] = df_train['dialogue'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# calculate statistics of dialogue lengths\n",
    "print(\"----- Dialogue Length Statistics -----\")\n",
    "print(f\"\\nAverage dialogue length: {df_train['dialogue_length'].mean():.2f} words\")\n",
    "print(f\"\\nMedian dialogue length: {df_train['dialogue_length'].median():.2f} words\")\n",
    "print(f\"\\nMinimum dialogue length: {df_train['dialogue_length'].min()} words\")\n",
    "print(f\"\\nMaximum dialogue length: {df_train['dialogue_length'].max()} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0lPYaJY2c4TK",
    "outputId": "de0c5027-4a64-4892-c212-e99f0eba8d90"
   },
   "outputs": [],
   "source": [
    "# create new column 'summary_length' in df_train by taking 'summary' column and splitting the string and finding its length\n",
    "df_train['summary_length'] = df_train['summary'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# calculate statistics of summary lengths\n",
    "print(\"----- Summary Length Statistics -----\")\n",
    "print(f\"\\nAverage summary length: {df_train['summary_length'].mean():.2f} words\")\n",
    "print(f\"\\nMedian summary length: {df_train['summary_length'].median():.2f} words\")\n",
    "print(f\"\\nMinimum summary length: {df_train['summary_length'].min()} words\")\n",
    "print(f\"\\nMaximum summary length: {df_train['summary_length'].max()} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9d10VyGxTtj5",
    "outputId": "dc838a12-d12c-409f-dc08-6f7dac8af0df"
   },
   "outputs": [],
   "source": [
    "# check for missing values in the dataframes\n",
    "print(f\"\\nMissing values in df_train:\\n\")\n",
    "print(f\"Missing values in df_train:\\n\")\n",
    "print(df_train.isnull().sum())\n",
    "\n",
    "print(f\"\\nMissing values in df_validation:\\n\")\n",
    "print(df_val.isnull().sum())\n",
    "\n",
    "print(f\"\\nMissing values in df_test:\\n\")\n",
    "print(df_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qNbMne5oVJgR",
    "outputId": "196a3161-186c-4601-f299-d452904d0feb"
   },
   "outputs": [],
   "source": [
    "# drop the row from df_train with missing value in dialogue column\n",
    "df_train = df_train.dropna(subset=['dialogue'])\n",
    "print(f\"Missing values in df_train:\\n\")\n",
    "print(df_train.isnull().sum())\n",
    "print(f\"\\nLength of train df: {len(df_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "id": "1XplI6u5ew2E",
    "outputId": "fc883d63-cd1b-4c82-eac8-223a803bea06"
   },
   "outputs": [],
   "source": [
    "# plot dialogue vs summary lengths for df_train\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df_train['dialogue_length'], bins=50, alpha=0.5, label='Dialogue Length')\n",
    "plt.hist(df_train['summary_length'], bins=50, alpha=0.5, label='Summary Length')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Distribution of Dialogue and Summary Lengths')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "0pQqbYEnez5I",
    "outputId": "cd7e3c0d-641a-4976-f0d7-4ee37d6e5183"
   },
   "outputs": [],
   "source": [
    "# plot dialogue, summary, and compression statistics for df_train\n",
    "df_train['compression'] = df_train['dialogue_length'] / df_train['summary_length']\n",
    "stats = df_train[['dialogue_length', 'summary_length', 'compression']].agg(['mean', 'median', 'min', 'max']).round(2).T\n",
    "\n",
    "# plot statistics\n",
    "stats.T.plot(kind='bar', figsize=(10,6))\n",
    "plt.title('Descriptive Statistics: Dialogue, Summary and Compression Ratio')\n",
    "plt.xlabel('Statistics')\n",
    "plt.ylabel('Value')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Metric')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rq2I9X3gYIfj",
    "outputId": "3c4f0834-9508-4a17-a489-be2fcae84111"
   },
   "outputs": [],
   "source": [
    "# examples of dialogue and summary from df_train\n",
    "\n",
    "print(f\"Dialogue #1 from df_train:\")\n",
    "print(f\"\\nDialogue:\\n{df_train['dialogue'][1]}\")\n",
    "print(f\"\\nSummary:\\n{df_train['summary'][1]}\")\n",
    "\n",
    "print(f\"\\nDialogue #2 from df_train:\")\n",
    "print(f\"\\nDialogue:\\n{df_train['dialogue'][1000]}\")\n",
    "print(f\"\\nSummary:\\n{df_train['summary'][1000]}\")\n",
    "\n",
    "print(f\"\\nDialogue #3 from df_train:\")\n",
    "print(f\"\\nDialogue:\\n{df_train['dialogue'][10000]}\")\n",
    "print(f\"\\nSummary:\\n{df_train['summary'][10000]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFIblUW7RoEW"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "78e16ae2754a41d9b29d5ccd23398450",
      "916f40be4b2742fd8b5f611e0dfcc73a",
      "0201fe5c1a8549ecb50ffb37ffaecda0",
      "b8a0cf2cfdd14c739ba68fe745a34de1",
      "7f7f0d473df2413182ca8c8fbe069294",
      "6c5b088ff1af4ea29e121bea5c3ddfbf",
      "ea99823720834cabb66fc495d279bcbe",
      "1238be36655e49a0ad96899165860405",
      "8b63422f4c664ab18c210332ad637979",
      "f34f1fbf192e478ea139538de498858f",
      "081537fc08914e45add33e5d6fb27085",
      "29471cd6405d4a2a9851927a6a46e089",
      "6db775450cfe4a2ca9967d49bc4d4641",
      "d37dfc65274f46258aa3a862581eee48",
      "eb36a750c4474724bc840016fd2ce95e",
      "a26601dcaf1e467ab8a7c961ad9daa14",
      "69962ed2829f40ff8f9c50cd58214d29",
      "fda9e52f49ee430aa9a05e3e3cd2563c",
      "a22cfe9aeaec418b982f6801bc135d67",
      "4965b5152c1b40e6a0a7f13493a2536f",
      "f81588f0126b4e2dba7916bfa2e30fd3",
      "33af15a3f0364a24bb7382bf36ac8ed8",
      "5e4ac7d156094f688df7dc59c0694d73",
      "30c9489564b64232bc789aab0f43295a",
      "4ff43414a5194d4594cab52c754250de",
      "3b3c164c59744f5ba162c88b35703a37",
      "dbbc1e4d442e48e590d060a538b1d2b3",
      "8216e34671154f65b1a52920881d55d4",
      "46494550bafb4ad390ac5761fd20834b",
      "f98c1cda958c41ccaaf0cd3cc12609f8",
      "0dbb3ca8bf3147dbb667ad955f2aa1ab",
      "b1fb0b4a32af451b996accc4456110b8",
      "e15fb7b7cd894e0da054b609f0fe552e"
     ]
    },
    "id": "WAv4n8vLco0H",
    "outputId": "97da6bef-a5bb-44d4-8b39-1a0b7da2d6c8"
   },
   "outputs": [],
   "source": [
    "# remove dialogues with fewer than 10 words\n",
    "min_dialogue_length = 10\n",
    "\n",
    "def is_long_enough(example):\n",
    "  return example['dialogue'] is not None and len(example['dialogue'].split()) >= min_dialogue_length\n",
    "\n",
    "# apply filter to each dataset split\n",
    "dataset['train'] = dataset['train'].filter(is_long_enough)\n",
    "dataset['validation'] = dataset['validation'].filter(is_long_enough)\n",
    "dataset['test'] = dataset['test'].filter(is_long_enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEuaqBQViBnx"
   },
   "source": [
    "Prep data for input to BERT model\n",
    "* implement appropriate tokenization\n",
    "* create training and validation splits\n",
    "* build data loaders for efficient model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "985f2e0d9f7044589996eb598682a981",
      "9f401235370b4d609fb79116aaff0a51",
      "e2b82fa6d5594f558cc4baad4017c104",
      "b6541d6a277b4f5e92ba47025d2dd1b0",
      "c3fc0b548d6f4b1e8c07a3c5c5e26c06",
      "0cff4bb9f12f403390a7a2cf4323196f",
      "d954edaf05ed435fafec753e11b1d11a",
      "8c693a4571cc4975b2acc4a7e98990e9",
      "48dd8cd16b9d4400bac614f70b028982",
      "467110ea08ca4c80a46a2d4dc0b26c0b",
      "7e2a1a7adf874962b2e815380a5b8e64",
      "6508f0ed64d1490e9911a91e5664ee81",
      "85541b50777d45da8b9ff07ad91a16a5",
      "2a8aef1f2fd9432ebbaea1ab9c07620e",
      "28666aa35e664584b3c10fa081a9ae1d",
      "d9c6eb65941b484290b7eaddb26c62ce",
      "582c5c09faad40418fb253b20e28b453",
      "14f5e9e415f14b7588f48d5560574264",
      "20531771455047adbf82620530126090",
      "937f6e245fa8494384b5c29da0acc327",
      "178c014935b949b4848c32d1b1ef22a3",
      "d58b6100f5414cd69405413b7f5edddd",
      "6316aba57c684e5fa0d24e0930f37da5",
      "b7d4673608af4d2ab51146def43c2917",
      "31d0123cdb0f470fab8f983cd8b63940",
      "abe040ff7665495ca46f261a150b310c",
      "2201debbe32048d7835b14739aa8e365",
      "1dd8fde3aad8400da084a30f7ae63e82",
      "b220cc3e4a5d4664bea187e60513e26f",
      "e920848cead1450892f02a1563fcd946",
      "01f67c40076c42a88492748469ecb321",
      "64f187e8ab6244eda74df29f64a15b81",
      "6a682e570306426f93f83a1f3831102e",
      "34ca1bf9621a4be8845d0e09f1fc54a2",
      "18791c14120843d0b6e743b8beb61b77",
      "c2c1907a94a74c12ae4869d1e8b7b886",
      "155d42b8589a40b19e473744cd578adf",
      "b01d6ac36b1747cb87d28485b83e5383",
      "05181a1216484621854109f273915f92",
      "984bd3fd6fc54ea09a79bb17c73776ad",
      "52ff88e1172f4cd497983db63ad74789",
      "326e65ac2a2e45fca209fd71acda1ef0",
      "0db572707dcb4917bad6bb7e9bb962a5",
      "bfbb1058e4b44459bf997b13ad0717fe"
     ]
    },
    "id": "_uTe2XWpRpBo",
    "outputId": "9d12e8ae-c66b-4b34-bbf2-f112db00e88b"
   },
   "outputs": [],
   "source": [
    "# initialize tokenizer\n",
    "# converts input dialogue into token IDs for the encoder\n",
    "# converts target summary into token IDs for the decoder\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 64\n",
    "\n",
    "# define tokenize function\n",
    "def tokenize_function(batch):\n",
    "\n",
    "  # tokenize input dialogue\n",
    "  inputs = tokenizer(\n",
    "      batch['dialogue'],\n",
    "      padding='max_length',\n",
    "      truncation=True,\n",
    "      max_length = MAX_INPUT_LENGTH,\n",
    "  )\n",
    "\n",
    "  # tokenize target summary\n",
    "  with tokenizer.as_target_tokenizer():\n",
    "    labels = tokenizer(\n",
    "        batch['summary'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "  )\n",
    "\n",
    "  # attach labels to inputs\n",
    "  inputs['labels'] = labels['input_ids']\n",
    "  return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217,
     "referenced_widgets": [
      "467c8a10becc4d969c134565b516f220",
      "34886ecbb64545d298928b1115f53421",
      "12c6adb937704b85bd89c692dff0b11d",
      "adae9f28bb2b4ed58f4e1c9805691241",
      "5cc83afe4e8c47cbb8d0b26e17a4c345",
      "0e35a55f1c8f4c46ab1345577e8dd273",
      "e7b856dfae3a4135a01549f649c53281",
      "6502c647cbfb4dcabb45a6a99fd50202",
      "62762c79ab1f4f8a915af04990c9c143",
      "1d741bc766e34ef8aebd03e7c7668063",
      "c22d793a841e4f0aa85af90f9de4a1c5",
      "692fc6b927204dc3aa73941db2dbed98",
      "77fa7edee3f44947925dfbbbca67206f",
      "14fdb2c04fbe4db2b380d43ea1b9834f",
      "ff4b2ee4e20046d6a033e483bcb06b16",
      "7029f4bd20aa4d3fbc7632e26f0b643a",
      "e5010ffe630c47b9b4ae4b7e3bb8361d",
      "721d9ea8c5a9460fad3ee20192c81319",
      "2c06c3e3a53e44eaa2ad6d5971344c1c",
      "b23c310a3bfb4d1385b44cda9dc08224",
      "deb2749c71c6462a868a0f990b3f7abf",
      "602021bb28874856a856fff74a53c51c",
      "58cfb91dcca14bb78612ede3bed9eadc",
      "48eebec3f2b141a886f25a78714d95aa",
      "930d643bd0934fb4a81fd0ef97778b64",
      "7a606cd5f96340099f355bcaec93ec67",
      "b4ecf7ce5fb640aa95f01248d086e23b",
      "22fa5b475a6e4cf7b3a1fec20920d90d",
      "d6f0721982944f75ba020676f0b03189",
      "df402702e61d48eea81b8993601bf92f",
      "05263d056b304e6eaf5548aa0f0fded0",
      "0dd1b5983e2f4c739942dd2ebd06b903",
      "f8b1c2320a274e629cfe1102551fced7",
      "b6db23978ccb4fd992576bddc60f146c",
      "e62acf9a49f84cdbaafd29f39b85a15d",
      "b74b6264a2224f3880e7efe448496707",
      "2d2d4fd94a32438498892f54a6e51a05",
      "572920888f0f46f6a538782a6ba91028",
      "d480a13cebe6468aa5034ae8245887d6",
      "9f10dfa282eb43118abf0e074abbb427",
      "e3f5518c39094b41b82bb30fdf8d566f",
      "b0366f374ca34d6ab74fbdf68003f94b",
      "98c62b0263e2415ebf60e18300b4489c",
      "2f8b0dbf61f9409296347facdd968602"
     ]
    },
    "id": "kPyKAX1s1_SS",
    "outputId": "0528820d-9879-4ba2-8325-062d34182b8c"
   },
   "outputs": [],
   "source": [
    "# tokenize train, validation, and test splits\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Remove examples with None in 'dialogue' or 'summary'\n",
    "dataset['train'] = dataset['train'].filter(lambda x: x['dialogue'] is not None and x['summary'] is not None)\n",
    "\n",
    "tokenized_train = dataset['train'].map(tokenize_function, batched=True)\n",
    "tokenized_val = dataset['validation'].map(tokenize_function, batched=True)\n",
    "tokenized_test = dataset['test'].map(tokenize_function, batched=True)\n",
    "\n",
    "# save tokenized datasets, instead of tokenizing every time, on PC\n",
    "# torch.save(tokenized_train, 'data/tokenized_train.pt')\n",
    "# torch.save(tokenized_val, 'data/tokenized_val.pt')\n",
    "# torch.save(tokenized_test, 'data/tokenized_test.pt')\n",
    "\n",
    "# save tokenized datasets to Google Drive\n",
    "save_path = '/content/drive/MyDrive/tokenized_data'\n",
    "import os\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "torch.save(tokenized_train, os.path.join(save_path, 'tokenized_train.pt'))\n",
    "torch.save(tokenized_val, os.path.join(save_path, 'tokenized_val.pt'))\n",
    "torch.save(tokenized_test, os.path.join(save_path, 'tokenized_test.pt'))\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Tokenization time: {(end_time - start_time)/60:.2f} minutes\")\n",
    "\n",
    "# Tokenization time: 50 sec on 7/25/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntHm_GJwjAe3"
   },
   "outputs": [],
   "source": [
    "# later load tokenized datasets from Google Drive\n",
    "# tokenized_train = torch.load('/content/drive/MyDrive/tokenized_data/tokenized_train.pt')\n",
    "# tokenized_val = torch.load('/content/drive/MyDrive/tokenized_data/tokenized_val.pt')\n",
    "# tokenized_test = torch.load('/content/drive/MyDrive/tokenized_data/tokenized_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185,
     "referenced_widgets": [
      "cbcf9daa30a24ec192ca605cc66df698",
      "655da61a26924882a965db8de0ae438a",
      "befd429c7a6e4ec59cf2663786adf06e",
      "a74c38d38aa2461c8a922851cb850f53",
      "565ef262a50f4dd6a6183998d0857516",
      "3b60470cc261402383b6c8f6b56f1b97",
      "9982995f57ad44a48f91f0ec9118a302",
      "117f56ef5d524198823b9741a37a5dac",
      "f1c88abda0bd47eab13be805fecb7f90",
      "eda645297ad64ab4a73c9ad13e015513",
      "71d30b0d7b5249469cd18957e9b70cc9",
      "10fc939774f74feab357d85939fdfa6e",
      "b343ad5d3c3e42838affa3f81a52d391",
      "cfe8d043cc2f4bb282f2105e91d0c673",
      "fa29bf6370b34bd382bee9bebb427332",
      "f9329fb3e59e4fe58c58c3b281ddcddf",
      "560c5a82683b4bc294c5635373a25f41",
      "b8f88ddc125849119c7d801088314a79",
      "5216b4e20ee24e598b4723f8e2ddf89e",
      "d5b50a81c9ee44fe82ae3360c7c0a78c",
      "27661a7dc84e48fd81f56eb66036a32f",
      "6973768d21e34c79a9aa7af513ac1f9d",
      "ad0626c9a8404961aa14a0920eced8de",
      "b4796cad89124455bbcb0beac5ed68c8",
      "9527d68638d34de3aa6b371d6069fe55",
      "9131c6ae855c4089bb9e7c7228db8268",
      "c6af6ee613fa40f1a41af7b33a325b3e",
      "0222d1fe57b346cc921949ae22e57313",
      "e9f48ecd2f574782b72098505929764e",
      "e0d454cd7cb94e349be38d62e5710f3b",
      "c4d1fe77d70a48a3969867ade72b2296",
      "63dd6d44e3924fa7a2286e99f3b44aa5",
      "4281a5fafed64ba683ef9357436a5514"
     ]
    },
    "id": "nl32B8e66wuZ",
    "outputId": "3dced50f-acc6-44b4-c598-7340365647fa"
   },
   "outputs": [],
   "source": [
    "# load a BERT2BERT pre-trained model (encoder-decoder)\n",
    "# use patrickvonplaten/bert2bert-cnn_dailymail-fp16 as a BERT2BERT model fine-tuned on CNN/DailyMail\n",
    "# encoder and decoder both use bert-case-uncased\n",
    "# bert-case-uncased is a commonly used pretrained BERT model from the original BERT paper\n",
    "\n",
    "model = EncoderDecoderModel.from_pretrained('patrickvonplaten/bert2bert-cnn_dailymail-fp16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "305-cNG5AID8"
   },
   "outputs": [],
   "source": [
    "# configure the model for the summarization task\n",
    "\n",
    "# start of decoding, for BERT based decoders, CLS is used\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "\n",
    "# end of sequence token, SEP used for BERT\n",
    "model.config.eos_token_id = tokenizer.sep_token_id\n",
    "\n",
    "# padding for batches\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# ensures encoder and decoder share the same vocab size\n",
    "model.config.vocab_size = model.config.encoder.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsaMdZU8RpyJ"
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "p2GSq5HHRqxr",
    "outputId": "68277f1b-a229-4a02-df6c-c27cdfeaa630"
   },
   "outputs": [],
   "source": [
    "# set up training arguments for model training (summarization)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # core argument\n",
    "    output_dir=\"./bert2bert_samsum\",    # save model checkpoints, logs, etc.\n",
    "    save_steps=500,         # save a checkpoint every 500 steps, for recovery or versioning\n",
    "    save_total_limit=2,     # keep only the last 2 saved models\n",
    "    logging_steps=500,      # log training loss and metrics every 500 steps\n",
    "    do_train=True,          # enable training\n",
    "    do_eval=True,           # enable eval during training on eval dataset\n",
    "    do_predict=True,        # make predictions after training\n",
    "\n",
    "    # training configuration\n",
    "    per_device_train_batch_size=4,  # batch size per device for training\n",
    "    per_device_eval_batch_size=4,   # batch size per device for evaluation\n",
    "    num_train_epochs=3,             # number of times the model will see the full dataset\n",
    "    learning_rate=5e-5,             # how quickly the model updates weights\n",
    "    lr_scheduler_type=\"cosine\",     # how learning rate changes over time\n",
    "                                    # linear gradually decreases, or cosine, constant\n",
    "    warmup_steps=500,               # num of steps to slowly ramp up learning rate\n",
    "                                    # before stabilizing. helps avoid bad initial updates\n",
    "\n",
    "    # performance optimization\n",
    "    fp16=True,              # reduce memory usage and speed up training\n",
    "\n",
    "    # monitoring and model selection\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,         # True if higher metric values are better (ROUGE)\n",
    "    report_to=\"tensorboard\",        # enables TensorBoard logging\n",
    ")\n",
    "\n",
    "# 2. Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# 3. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6r7SE622sCbj",
    "outputId": "133812c5-5334-4ea7-dd2f-d9f3a23bc5f7"
   },
   "outputs": [],
   "source": [
    "# train the model and log the training process with MLflow\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "mlflow.set_experiment(\"BERT2BERT SAMSum\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"model\", \"bert2bert-cnn_dailymail-fp16\")\n",
    "    mlflow.log_param(\"epochs\", 3)\n",
    "    mlflow.log_param(\"batch_size\", 4)\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "    # Log final model and tokenizer to PC\n",
    "    # model.save_pretrained(\"./bert2bert_samsum_final\")\n",
    "    # tokenizer.save_pretrained(\"./bert2bert_samsum_final\")\n",
    "\n",
    "    # save model to google drive\n",
    "    model.save_pretrained(\"/content/drive/MyDrive/bert2bert_samsum_final\")\n",
    "    tokenizer.save_pretrained(\"/content/drive/MyDrive/bert2bert_samsum_final\")\n",
    "\n",
    "    # log to MLflow for experiment tracking on PC\n",
    "    # mlflow.log_artifacts(\"./bert2bert_samsum_final\", artifact_path=\"model\")\n",
    "\n",
    "    # log to MLflow for experiment tracking on Google Drive\n",
    "    mlflow.log_artifacts(\"/content/drive/MyDrive/bert2bert_samsum_final\", artifact_path=\"model\")\n",
    "\n",
    "    # # save to Google Drive to avoid data loss\n",
    "    # !cp -r ./bert2bert_samsum_final /content/drive/MyDrive/bert2bert_samsum_final\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {(end_time - start_time)/60:.2f} minutes\")\n",
    "\n",
    "# training time: 51 minutes on 7/25/25\n",
    "# training time: 42 minutes on 7/26/25 w/ 3 epochs\n",
    "# training time: 71 min on 7/26/25 w/ 3 epochs (from MLflow)\n",
    "# training time: 66.8 min on 7/27/25 w/ 5 epochs\n",
    "# training time: 67 min on 7/27/25 w/ 10 epochs\n",
    "# training time: 40 min on 7/27/25 w/ 3 epochs\n",
    "# training time: 47 min on 7/29/25 w/ 3 epochs\n",
    "# training time: 40 min on 7/29/25 w/ 3 epochs and batch size 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHVPDdaG6Y7-"
   },
   "source": [
    "view MLflow UI by running:\n",
    "mlflow ui\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msDiYges3vWT"
   },
   "outputs": [],
   "source": [
    "# reload model and tokenizer from PC\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"./bert2bert_samsum_final\")\n",
    "# model = EncoderDecoderModel.from_pretrained(\"./bert2bert_samsum_final\")\n",
    "\n",
    "# reload model and tokenizer form google drive\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/content/drive/MyDrive/bert2bert_samsum_final\")\n",
    "model = EncoderDecoderModel.from_pretrained(\"/content/drive/MyDrive/bert2bert_samsum_final\")\n",
    "\n",
    "# move model to device (GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "y6OGnx5_3vWU",
    "outputId": "e69233c4-eb90-4089-d4ab-9bcd822c0ed9"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Redefine training arguments (can extend epochs)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert2bert_samsum\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=500,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,  # increase this for additional epochs\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"cosine\",   # try linear and cosine\n",
    "    warmup_steps=500,\n",
    "    fp16=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Redefine data collator and trainer\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,  # should be already in memory\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()  # resume training from last checkpoint\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training time: {(end_time - start_time)/60:.2f} minutes\")\n",
    "\n",
    "# training time: 67 min on 7/27/25 w/ 10 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zu8n93ANRtKv"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEE4_WqTsCbj"
   },
   "source": [
    "use ROUGE (Recall-Oriented Understudy for Gisting Evaluation) to compare the overlap between generated summaries and reference summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dI_Y6XQzsCbj",
    "outputId": "f66c1869-a6fb-4865-b6cd-4a2a6b6c01ef"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "# inspect how well the model summarizes dialogues from the test set\n",
    "\n",
    "# Get 5 evenly spaced indices from the test set\n",
    "indices = np.linspace(0, len(dataset['test']) - 1, num=5, dtype=int)\n",
    "\n",
    "# Cast each to Python int explicitly\n",
    "raw_samples = [dataset['test'][int(i)] for i in indices]\n",
    "\n",
    "# Tokenize the dialogues\n",
    "inputs = tokenizer(\n",
    "    [sample['dialogue'] for sample in raw_samples],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    ").to(model.device)\n",
    "\n",
    "# Generate summaries with trained model\n",
    "output_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_length=64,                # already used\n",
    "    min_length=20,                # optional, for preventing too short outputs\n",
    "    length_penalty=1.2,           # >1.0 discourages long outputs\n",
    "    num_beams=4,                  # better than greedy (1)\n",
    "    early_stopping=True,          # stop when an EOS token is reached\n",
    "    no_repeat_ngram_size=3        # prevents repetition\n",
    ")\n",
    "\n",
    "# Decode output tokens into readable text\n",
    "decoded_preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "decoded_labels = [sample['summary'] for sample in raw_samples]\n",
    "\n",
    "# Show prediction (generated summaries) vs. reference (ground truth summaries)\n",
    "for i in range(len(decoded_preds)):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Dialogue:\\n{raw_samples[i]['dialogue']}\")\n",
    "    print(f\"\\nGenerated Summary:\\n{decoded_preds[i]}\")\n",
    "    print(f\"\\nReference Summary:\\n{decoded_labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2TgT26eOQyy",
    "outputId": "ad422519-0b7f-4ce9-f735-15f4a232a762"
   },
   "outputs": [],
   "source": [
    "# add progress bar to see how many iterations are left\n",
    "from tqdm import tqdm\n",
    "\n",
    "decoded_preds = []\n",
    "decoded_labels = []\n",
    "\n",
    "# loop over test dataset, one dialogue at a time\n",
    "for idx in tqdm(range(len(dataset['test']))):\n",
    "    # Get one dialogue and ground truth summary\n",
    "    dialogue = dataset['test'][int(idx)]['dialogue']\n",
    "    reference = dataset['test'][int(idx)]['summary']\n",
    "\n",
    "    # tokenize the dialogue and convert into input tensors\n",
    "    inputs = tokenizer(dialogue, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
    "\n",
    "    # Generate one summary\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_length=64,\n",
    "            min_length=20,\n",
    "            length_penalty=1.2,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "\n",
    "    # Decode output and store\n",
    "    summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds.append(summary)\n",
    "    decoded_labels.append(reference)\n",
    "\n",
    "    # clear memory after each loop to avoid CUDA memory issues\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188,
     "referenced_widgets": [
      "3ffc59fda7a1400e97907b633cfb334c",
      "60770b7eb6404a68998539f89eea6aed",
      "dc5f090437ec45e59f74ddac1e854746",
      "b93d3bf7813b436a8afcb2689c635918",
      "91724b1a233f4a708019083e7ed549a7",
      "5f02a7e004004ed8a8e2f73a584a3e87",
      "6f06551a1c8746d79dbb45028340097c",
      "d69b65a64f7b4b97a17789d17e02b775",
      "f2e181b6144841fdb3477314ae3205da",
      "37d869a023d047ada676a7ad8000a1e0",
      "8f921cd02b45407489d1eefddb352f2d"
     ]
    },
    "id": "IjLbZzBPRt8-",
    "outputId": "e9f82de7-b9c2-4242-b2f1-59d0c10ddcf3"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# load ROUGE evaluation model from evaluate library\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "# Clean predictions and labels\n",
    "decoded_preds_clean = [pred.strip() for pred in decoded_preds]\n",
    "decoded_labels_clean = [label.strip() for label in decoded_labels]\n",
    "\n",
    "# Compute ROUGE scores\n",
    "# ROUGE-1 = unigram overlap\n",
    "# ROUGE-2 = bigram overlap\n",
    "# ROUGE-L = longest common subsequence\n",
    "# ROUGE-Lsum = ROUGE-L for summaries\n",
    "results = rouge.compute(predictions=decoded_preds_clean, references=decoded_labels_clean)\n",
    "print(\"Rouge scores for BERT2BERT model:\\n\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nEvaluation time: {(end_time - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a65ThhW83vWs",
    "outputId": "51ffffd0-97b7-41ae-9cbb-455cf51169a0"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Add this import if not already present\n",
    "\n",
    "def generate_summaries_in_batches(dialogues=None, batch_size=8):\n",
    "    if dialogues is None:\n",
    "        dialogues = dataset['test']['dialogue']\n",
    "    generated = []\n",
    "    model.eval()  # switch to inference mode\n",
    "\n",
    "    # Wrap the loop with tqdm for progress monitoring\n",
    "    for i in tqdm(range(0, len(dialogues), batch_size), desc=\"Generating summaries\"):\n",
    "        batch = dialogues[i:i + batch_size]\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_length=64,\n",
    "                min_length=20,\n",
    "                num_beams=4,\n",
    "                no_repeat_ngram_size=3,\n",
    "                early_stopping=True\n",
    "            )\n",
    "\n",
    "        batch_summaries = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        generated.extend(batch_summaries)\n",
    "\n",
    "        # optional: free GPU memory between batches\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return generated\n",
    "\n",
    "generated_summaries = generate_summaries_in_batches(batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "id": "tNJgblLh3vWt",
    "outputId": "460af250-69ec-4532-c776-b9b0d78d6e62"
   },
   "outputs": [],
   "source": [
    "# visualize test dialogue vs summary vs generated summary lengths\n",
    "\n",
    "# Compute lengths\n",
    "test_dialogue_lengths = [len(d.split()) for d in dataset['test']['dialogue']]\n",
    "test_ref_summary_lengths = [len(s.split()) for s in dataset['test']['summary']]\n",
    "\n",
    "# model outputs\n",
    "test_gen_summary_lengths = [len(s.split()) for s in generated_summaries]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(test_dialogue_lengths, bins=50, alpha=0.4, label='Dialogue Length')\n",
    "plt.hist(test_ref_summary_lengths, bins=50, alpha=0.5, label='Reference Summary Length')\n",
    "plt.hist(test_gen_summary_lengths, bins=50, alpha=0.6, label='Generated Summary Length')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.title('Test Set: Dialogue vs. Summary vs. Generated Summary Lengths')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mA3HmCX36Y8C"
   },
   "source": [
    "Visually, the generated summaries are similar in length to the reference summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTYeE4go3vWu"
   },
   "outputs": [],
   "source": [
    "# # Compute word lengths\n",
    "# test_dialogue_length = [len(d.split()) for d in dataset['test']['dialogue']]\n",
    "# test_summary_length = [len(s.split()) for s in dataset['test']['summary']]\n",
    "# test_generated_length = [len(s.split()) for s in generated_summaries]\n",
    "\n",
    "# Compute compression ratios\n",
    "test_compression_ref = [d/s if s != 0 else 0 for d, s in zip(test_dialogue_lengths, test_ref_summary_lengths)]\n",
    "test_compression_gen = [d/s if s != 0 else 0 for d, s in zip(test_dialogue_lengths, test_gen_summary_lengths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 431
    },
    "id": "s4BDTlsL3vWu",
    "outputId": "6eaf9dc8-0b66-4f54-8c2f-964bb240f4d2"
   },
   "outputs": [],
   "source": [
    "# visualize test dialogue vs summary vs generated summary lengths and compression ratios\n",
    "df_stats = pd.DataFrame({\n",
    "    \"Dialogue Length\": test_dialogue_lengths,\n",
    "    \"Reference Summary Length\": test_ref_summary_lengths,\n",
    "    \"Generated Summary Length\": test_gen_summary_lengths,\n",
    "    \"Compression (Reference)\": test_compression_ref,\n",
    "    \"Compression (Generated)\": test_compression_gen\n",
    "})\n",
    "\n",
    "# Aggregate statistics\n",
    "summary_stats = df_stats.agg(['mean', 'median', 'min', 'max']).round(2).T\n",
    "\n",
    "summary_stats.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Test Set: Dialogue, Summary, and Compression Statistics')\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Value')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Statistic')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "8cde0f9ae8344911906a861f7fa4187d",
      "24a32b5ab99a48f1b8b016eeb4bd475e",
      "95bea2485e514837af8193fa5d3588c4",
      "1c355bdca7c8454e8909428c181ec48c",
      "62f16c772e6542f19666e214cd0e9611",
      "1c2e59b58c0f4609abd5fb590c77edfd",
      "e7e3146c04434e1c8dfed34f2db859f9",
      "a90468b8071a40a4afff98175cc8802a",
      "d72611d8a9594f8cb0aa730214efb8c9",
      "83a0bd8a431c4917b4879518dd142819",
      "0092e8e01f6c47c28c7ae65f01949a7f",
      "4efcd50cfc074cb8ac883375e8e3380b",
      "b31701e9f13a4c0abe1394611ca6f97a",
      "da81c91f1f37419db824b383718b59b6",
      "06b4199366a44e2ca54a0ab413986f03",
      "cccaa80fb22b4fb5a34520787e539e15",
      "b6cf5dad1eb640e79300a32a12f62e9f",
      "83a8162d2001410394de1b5e483bc3b5",
      "9991588616414bed9dd9b6394623047e",
      "1515fe32867e4a72b34d954a39c21a06",
      "e0f50f66bf074383902e6ea7b9590c2d",
      "c9cf831c0dca4f9bb2147157d3f4e099",
      "74918906ffe542358a850c3bdf2ed847",
      "27a8ed373d394fa3845a3ef5a2c42372",
      "85ece7a7f868413582e4ac20ac2c3960",
      "9669e27cf74d4e0b892873aa634a53fa",
      "8cd0e1d4ffe349b0ab7f43e037ba7195",
      "9240aaa383ed49f1857ec4337b01e148",
      "cc6b0cb980234f4a918309943ae14824",
      "ffe57e6eaf964ca59c1ccdcc27b58d26",
      "2922c9ac644d40b8b4c84f5f401a0a36",
      "b3980cd283754e2abaf207553ebf0a69",
      "05a8ad36c6d74842aecc076908cc3bc8",
      "188af0e8aa4e474d869021c816557648",
      "ed8ea691c08e4ca687ba1ad1de538398",
      "cb3d4d13d7d444c389322396b7d0ea33",
      "0d5fd9bd4d91486082d8caadf730b4b6",
      "ca67506c32e74b349a1e04eb81234ef1",
      "a53b69030509460da4cc03dc3cdd70cc",
      "2f2cec459c3d4445b88b950d1aa61ae3",
      "e40fb7e664394b18afc9528303729f1f",
      "0913de11228d4f299bc50dce5cc85cfa",
      "40fe221cdd484fe8a93cc8b6825dbfc0",
      "d83630fb49724416a11b245dff3a4470",
      "3b35deacda0c49f89e4cefe463a85a34",
      "e75604686bc44361a11ddbc6af3132d7",
      "1972f13c7599437785bdf03cf7184d08",
      "3ae820515c1d46a3baad438c91fa72c4",
      "2fea8c6ec16744daad33dd5ada4d2fcc",
      "a05f99a8f3204aa88ed1dabfb383ddda",
      "136edf8534a447589c7ac413c148a1e0",
      "ea557ee073a1496aa41309fe9e7487f0",
      "b177b8fa2bbe40cfbb51bc319213d26a",
      "31da902d5b674d98847ab30993e90aea",
      "5781165cf18149bca525b16bdd9aa45f",
      "9d68cf9990ae40d0987ed8bc3391b39e",
      "13f4a36f6a9048f28984a9da5327e3da",
      "2a811470c9774ce6b799a7fb03c67f24",
      "f20a1817f3c44da6b6afa5d74f05a0c4",
      "fe9e0853dd464a5ebb3f4d075d991648",
      "75d2abe117294cefb607c4c9a3e84e9c",
      "dd8ff7e7b0f84487bef8c940912b058b",
      "09fc5f66a9984ba99fc9aeed7598a672",
      "2b3a8551b587447d8d9af6dc2d2e77d2",
      "3a2e323b931b4941af88ff60d28ee378",
      "b8e14cba9e7f44a2a958cd5e7e1d274c",
      "1f3bde6306ea47939f1809b751b00495",
      "8bbbde20fde64440afe15841b6b76c4c",
      "e9780adb80eb4dda98166fee07569d9d",
      "cc96e2ae0d444543b6852727fd57d34f",
      "34f25411995a4334bc1e6e8a79ded941",
      "410353aec8e3455193a3f0a56c89e706",
      "e5b44adaa05c4575bdfcfdf48d1986c8",
      "b0c42c4b5ec140d38bf9ee4b337a058a",
      "47f213486de34caabc43c52f2e34254c",
      "0590df5c634643049117504d55ae9fc5",
      "1c73e768fac44dc6bfb73bd791fff565"
     ]
    },
    "id": "tlh8Mtm3sCbj",
    "outputId": "b699c285-c979-4fc7-d4c5-57387aa04a61"
   },
   "outputs": [],
   "source": [
    "# load seddiktrk/pegasus-samsum\n",
    "\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "model_pegasus = \"seddiktrk/pegasus-samsum\"\n",
    "\n",
    "tokenizer_pegasus = PegasusTokenizer.from_pretrained(model_pegasus)\n",
    "model_pegasus = PegasusForConditionalGeneration.from_pretrained(model_pegasus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jnPnLRy96Y8D",
    "outputId": "26c4e85a-30f2-4297-ace3-458c5ee35fbf"
   },
   "outputs": [],
   "source": [
    "# Pick a sample dialogue\n",
    "dialogue = dataset['test'][42]['dialogue']\n",
    "reference = dataset['test'][42]['summary']\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer_pegasus(dialogue, return_tensors=\"pt\", truncation=True, padding=True).to(model_pegasus.device)\n",
    "\n",
    "# Generate summary\n",
    "summary_ids = model_pegasus.generate(\n",
    "    **inputs,\n",
    "    max_length=64,\n",
    "    min_length=20,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Decode output\n",
    "generated_summary = tokenizer_pegasus.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Show result\n",
    "print(\"Dialogue:\\n\", dialogue)\n",
    "print(\"\\nReference Summary:\\n\", reference)\n",
    "print(\"\\nGenerated Summary (Pegasus):\\n\", generated_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KMep2icB6Y8D",
    "outputId": "3685a537-53cf-49f9-ef41-18b84712ca3b"
   },
   "outputs": [],
   "source": [
    "def generate_pegasus_batch(dialogues, batch_size=8):\n",
    "    summaries = []\n",
    "    model_pegasus.eval()\n",
    "    for i in tqdm(range(0, len(dialogues), batch_size), desc=\"Pegasus summaries\"):\n",
    "        batch = dialogues[i:i+batch_size]\n",
    "        inputs = tokenizer_pegasus(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model_pegasus.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model_pegasus.generate(\n",
    "                **inputs,\n",
    "                max_length=64,\n",
    "                min_length=20,\n",
    "                num_beams=4,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        decoded = tokenizer_pegasus.batch_decode(outputs, skip_special_tokens=True)\n",
    "        summaries.extend(decoded)\n",
    "        torch.cuda.empty_cache()\n",
    "    return summaries\n",
    "\n",
    "pegasus_summaries = generate_pegasus_batch(dataset['test']['dialogue'], batch_size=2)\n",
    "pegasus_rouge = rouge.compute(\n",
    "    predictions=[s.strip() for s in pegasus_summaries],\n",
    "    references=[s.strip() for s in dataset['test']['summary']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dD6Wcgit6Y8E",
    "outputId": "1267fc0e-49a2-4c18-9f0f-b0a876957ae2"
   },
   "outputs": [],
   "source": [
    "# print pegasus rouge scores\n",
    "print(\"ROUGE scores for Pegasus model:\\n\")\n",
    "for key, value in pegasus_rouge.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "VsiDd6FS6Y8E",
    "outputId": "7ba0389a-d838-4039-a1d2-814f8d89a4f9"
   },
   "outputs": [],
   "source": [
    "# BERT2BERT full test set ROUGE\n",
    "bert_rouge = rouge.compute(predictions=decoded_preds_clean, references=decoded_labels_clean)\n",
    "\n",
    "# Pegasus ROUGE\n",
    "pegasus_rouge = pegasus_rouge\n",
    "\n",
    "# Combine into a DataFrame\n",
    "rouge_df = pd.DataFrame({\n",
    "    'BERT2BERT': bert_rouge,\n",
    "    'Pegasus': pegasus_rouge\n",
    "})\n",
    "# Transpose for better plotting\n",
    "rouge_df.T.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title(\"ROUGE Score Comparison: BERT2BERT vs. Pegasus\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title=\"ROUGE Type\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xNlrCOEsCbj"
   },
   "source": [
    "# Notes on model performance\n",
    "# - Model captures structure of conversations well.\n",
    "# - Struggles with informal/abrupt phrases or very long dialogues.\n",
    "# - Sometimes over-summarizes or paraphrases too freely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dXfxx4WRw5H"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEPycPbmRylw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
